{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09c86bd",
   "metadata": {},
   "source": [
    "# Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf7fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PEFT along with dependencies\n",
    "!pip install -q peft transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b1ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import platform\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb53688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME\n",
      "/home/hice1/yhsu72/scratch/hf_cache\n",
      "TRITON_CACHE_DIR\n",
      "/home/hice1/yhsu72/scratch/triton_cache\n",
      "TORCHINDUCTOR_CACHE_DIR\n",
      "/home/hice1/yhsu72/scratch/inductor_cache\n",
      "NLTK_DATA\n",
      "/home/hice1/yhsu72/scratch/nltk_data\n",
      "Make sure the cache files are in ~/scratch/ so quota doesn't exceed limit!\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL ONLY IF RUNNING ON PACE-ICE\n",
    "# override the huggingface cache path and nltk cache path\n",
    "dirs = {\n",
    "    \"HF_HOME\":\"~/scratch/hf_cache\",\n",
    "    \"TRITON_CACHE_DIR\":\"~/scratch/triton_cache\",\n",
    "    \"TORCHINDUCTOR_CACHE_DIR\":\"~/scratch/inductor_cache\",\n",
    "    'NLTK_DATA':\"~/scratch/nltk_data\"\n",
    "}\n",
    "\n",
    "for name in dirs:\n",
    "    d = dirs[name]\n",
    "    path = os.path.expanduser(d)\n",
    "    print(name)\n",
    "    print(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    # making sure the cache dirs are rwx for owner\n",
    "    os.chmod(path, 0o700)\n",
    "    os.environ[name] = path\n",
    "print(\"Make sure the cache files are in ~/scratch/ so quota doesn't exceed limit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your own token (or save in .env)\n",
    "os.environ['HF_TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e2255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hugging Face token loaded from environment.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # loads HF_TOKEN into environment\n",
    "print(\"‚úÖ Hugging Face token loaded from environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f31a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== üß† Environment Info ===\n",
      "Python version: 3.10.13\n",
      "PyTorch version: 2.8.0+cu128\n",
      "-----------------------------\n",
      "‚úÖ CUDA is available. Number of GPUs: 2\n",
      "  ‚Ä¢ GPU 0: NVIDIA H200 (139.80 GB VRAM)\n",
      "  ‚Ä¢ GPU 1: NVIDIA H200 (139.80 GB VRAM)\n",
      "\n",
      "Using GPU: NVIDIA H200\n",
      "Available VRAM: 149.56 GB / 150.11 GB\n",
      "-----------------------------\n",
      "Default torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"=== üß† Environment Info ===\")\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# Check for CUDA (NVIDIA GPUs)\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"‚úÖ CUDA is available. Number of GPUs: {num_gpus}\")\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        print(f\"  ‚Ä¢ GPU {i}: {gpu_name} ({total_mem:.2f} GB VRAM)\")\n",
    "\n",
    "    # Also show current GPU and free memory\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    print(f\"\\nUsing GPU: {torch.cuda.get_device_name(current_gpu)}\")\n",
    "    free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "    print(f\"Available VRAM: {free_mem/1e9:.2f} GB / {total_mem/1e9:.2f} GB\")\n",
    "\n",
    "# Check for Apple Silicon (MPS)\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"‚úÖ Running on Apple Silicon (MPS backend).\")\n",
    "\n",
    "# Check for ROCm (AMD GPUs)\n",
    "elif torch.version.hip is not None:\n",
    "    print(\"‚úÖ ROCm (AMD GPU) detected.\")\n",
    "\n",
    "# Otherwise fallback to CPU\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected ‚Äî running on CPU only.\")\n",
    "    print(\"This will be very slow for large models like Llama-3.1-8B.\")\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# Confirm torch default device\n",
    "default_device = \"cuda\" if torch.cuda.is_available() else (\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Default torch device: {default_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa0d83b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acb801048cf444baf9b9e5f5a9457dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Model name on Hugging Face Hub ---\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# --- 4. (Optional) Authenticate if model is gated/private ---\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"YOUR_HF_TOKEN\")\n",
    "\n",
    "print(\"Loading tokenizer and model‚Ä¶\")\n",
    "\n",
    "# --- 5. Load tokenizer ---\n",
    "# Tokenizer converts text ‚Üî tokens. Must match model for correct vocabulary.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # ensure padding works\n",
    "\n",
    "# --- 6. Load model in full bf16 precision (no quantization) ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",      # Automatically distributes layers across GPUs\n",
    "    torch_dtype=torch.bfloat16,  # Use bf16 for all layers\n",
    "    low_cpu_mem_usage=True,       # Stream weights directly to GPU to reduce CPU RAM footprint\n",
    "    trust_remote_code=True        # Needed if the repo includes custom code\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7a0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Explain the difference between left-wing and right-wing economic policies.\n",
      "\n",
      "### Response: \n",
      "\n",
      "In a democratic society, people are free to express their opinions on political issues. However, there is no consensus on what constitutes ‚Äúleft‚Äù or ‚Äúright.‚Äù In general terms, however, we can say that those who advocate for greater government control of the economy tend toward leftist positions while conservatives argue against such regulation by advocating private enterprise as opposed to state interventionism.\n",
      "\n",
      "\n",
      "For instance, consider how different approaches towards taxation might represent divergent ideologies within this spectrum; libertarians believe strongly in minimal taxes since they view excessive governmental spending (particularly welfare programs) negatively whereas social democrats want higher tax rates because they see these funds being used wisely through public services like education which benefits everyone regardless income level rather than just some individuals at great cost when compared solely against\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 8. Simple inference test ---\n",
    "prompt = \"\"\"### Instruction:\n",
    "Explain the difference between left-wing and right-wing economic policies.\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,                  # control output length\n",
    "    do_sample=True,                      # enables some randomness\n",
    "    temperature=0.7,                     # mild creativity\n",
    "    top_p=0.9,                           # nucleus sampling\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.2               # prevent repeated text\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165a05c",
   "metadata": {},
   "source": [
    "# Part 1: Load & Prepare Data\n",
    "Data when loaded in:\n",
    "```\n",
    "{\n",
    "    \"instruction\": \"Write a political news story from a {left/ right/ center} perspective based on the headline.\", \n",
    "    \"input\": \"Headline: ...\",\n",
    "    \"output\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Steps to process it:**\n",
    "1. Combine into 1 traning text\n",
    "2. Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfb5e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded train.jsonl successfully!\n",
      "Number of samples: 2852\n",
      "\n",
      "üìä Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a political news story from a right pers...</td>\n",
       "      <td>Headline: If Democrats Flip House What Will Th...</td>\n",
       "      <td>Quotes displayed in real-time or delayed by at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a political news story from a center per...</td>\n",
       "      <td>Headline: Putin Marks Russias Victory Day Spee...</td>\n",
       "      <td>It has become an annual event - the military p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write a political news story from a left persp...</td>\n",
       "      <td>Headline: Media Industry Ap Criticized After D...</td>\n",
       "      <td>Copyright 2025 The Associated Press. All Right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Write a political news story from a left persp...</td>\n",
       "      <td>Headline: Economy And Jobs Will There Be Reces...</td>\n",
       "      <td>As the Fed wrestles with inflation, experts wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write a political news story from a right pers...</td>\n",
       "      <td>Headline: Technology Plans Tesla Tunnel Nashvi...</td>\n",
       "      <td>The project will be similar to one already in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Write a political news story from a right pers...   \n",
       "1  Write a political news story from a center per...   \n",
       "2  Write a political news story from a left persp...   \n",
       "3  Write a political news story from a left persp...   \n",
       "4  Write a political news story from a right pers...   \n",
       "\n",
       "                                               input  \\\n",
       "0  Headline: If Democrats Flip House What Will Th...   \n",
       "1  Headline: Putin Marks Russias Victory Day Spee...   \n",
       "2  Headline: Media Industry Ap Criticized After D...   \n",
       "3  Headline: Economy And Jobs Will There Be Reces...   \n",
       "4  Headline: Technology Plans Tesla Tunnel Nashvi...   \n",
       "\n",
       "                                              output  \n",
       "0  Quotes displayed in real-time or delayed by at...  \n",
       "1  It has become an annual event - the military p...  \n",
       "2  Copyright 2025 The Associated Press. All Right...  \n",
       "3  As the Fed wrestles with inflation, experts wo...  \n",
       "4  The project will be similar to one already in ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# build full paths\n",
    "data_path = os.path.join(cwd, \"train.jsonl\")   # input file\n",
    "# Option 1: Load using pandas ‚Äî easiest for inspection\n",
    "train_df = pd.read_json(data_path, lines=True)\n",
    "\n",
    "print(\"‚úÖ Loaded train.jsonl successfully!\")\n",
    "print(f\"Number of samples: {len(df)}\\n\")\n",
    "print(\"üìä Preview:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e536f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    \"\"\"\n",
    "    Combine instruction, input, and output into one training text.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        f\"### Instruction:\\n{example['instruction'].strip()}\\n\\n\"\n",
    "        f\"### Input:\\n{example['input'].strip()}\\n\\n\"\n",
    "        f\"### Response:\\n{example['output'].strip()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c37608d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Write a political news story from a right perspective based on the headline.\n",
      "\n",
      "### Input:\n",
      "Headline: If Democrats Flip House What Will They Prioritize\n",
      "\n",
      "### Response:\n",
      "Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided byFactset.\n",
      "          Powered and implemented byFactSet Digital Solutions.Legal Statement.\n",
      "\n",
      "This material may not be published, broadcast, rewritten, or redistributed. ¬©2025 FOX News Network, LLC. All rights reserved.FAQ-New Privacy Policy\n",
      "\n",
      "If the Democrats win a majority in theHouse, it will be Democrats who chair key committees. And since the chair is picked largely on the basis of seniority, we know who they will be.\n",
      "\n",
      "So let me introduce the Democrats who will become very powerful, if the Democrats win next week.\n",
      "\n",
      "Maxine Wate\n"
     ]
    }
   ],
   "source": [
    "# Apply formatting to DataFrames\n",
    "train_df[\"text\"] = train_df.apply(format_prompt, axis=1)\n",
    "\n",
    "# Preview one formatted sample\n",
    "print(train_df[\"text\"].iloc[0][:800])  # show first 800 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29704e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f5a8905f514287889e57cd43fad885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2852 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2852\n",
      "})\n",
      "============ Sanity Check - decoding encoded tokens ============\n",
      "<|begin_of_text|>### Instruction:\n",
      "Write a political news story from a right perspective based on the headline.\n",
      "\n",
      "### Input:\n",
      "Headline: If Democrats Flip House What Will They Prioritize\n",
      "\n",
      "### Response:\n",
      "Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided byFactset.\n",
      "          Powered and implemented byFactSet Digital Solutions.Legal Statement.\n",
      "\n",
      "This material may not be published, broadcast, rewritten, or redistributed. ¬©2025 FOX News Network, LLC. All rights reserved.FAQ-New Privacy Policy\n",
      "\n",
      "If the Democrats win a majority in theHouse, it will be Democrats who chair key committees. And since the chair is picked largely on the basis of seniority, we know who they will be.\n",
      "\n",
      "So let me introduce the Democrats who will become very powerful, if the Democrats win next week.\n",
      "\n",
      "Maxine Waters will chair the Financial Services Committee. Known for her slogan \"Impeach 45\" and encouraging confrontation with Republicans. She would be in charge of banking regulations!\n",
      "\n",
      "New Yorker\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,   # adjust based on GPU memory\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "# Convert to HF Dataset and tokenize\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\"]])\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Check shape and sample\n",
    "print(tokenized_train)\n",
    "\n",
    "print(\"============ Sanity Check - decoding encoded tokens ============\")\n",
    "print(tokenizer.decode(tokenized_train[0][\"input_ids\"][:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaffa87",
   "metadata": {},
   "source": [
    "# Part 2: LoRA fine-tuning\n",
    "**Steps**\n",
    "1. Set up LoRA model\n",
    "2. Set up training arguments\n",
    "3. Prepare a Data Collator\n",
    "4. Set up Trainer\n",
    "5. Run Training\n",
    "6. Save new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e6b975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "# Set up Lora model for fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                # rank of the LoRA matrices\n",
    "    lora_alpha=32,       # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # which layers to fine-tune\n",
    "    lora_dropout=0.05,   # dropout for LoRA\n",
    "    bias=\"none\",         # keep bias frozen\n",
    "    task_type=\"CAUSAL_LM\" # type of task\n",
    ")\n",
    "\n",
    "# Wrap base model with PEFT\n",
    "model = get_peft_model(model, lora_config) # freezes original layer\n",
    "model.print_trainable_parameters()  # confirm only LoRA params are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bb4a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments set up\n"
     ]
    }
   ],
   "source": [
    "# Define Training Arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/lora\",         # where to save checkpoints & LoRA adapters\n",
    "    per_device_train_batch_size=8,     # batch size per GPU\n",
    "    gradient_accumulation_steps=4,     # effective batch size = 8 * 4 * 2 GPUs = 64\n",
    "    learning_rate=3e-4,                # LoRA-friendly default\n",
    "    num_train_epochs=3,                # 3 epochs, increase if dataset is larger\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,                          # optional: H200 supports bf16, can improve speed\n",
    "    dataloader_drop_last=True,           # drop incomplete batch to avoid OOM\n",
    "    report_to=\"none\",                    # change to 'wandb' if using W&B\n",
    "    remove_unused_columns=False,\n",
    "    ddp_find_unused_parameters=False,   # improves multi-GPU performance\n",
    "    gradient_checkpointing=True,        # reduce memory usage for large models\n",
    "    warmup_steps=50,                    # optional warmup\n",
    "    optim=\"paged_adamw_32bit\",          # memory-efficient optimizer\n",
    "    lr_scheduler_type=\"cosine\"          # smooth LR schedule\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training arguments set up\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31533576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data collator ready\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data Collator (Coverts list of examples into tensors)\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Collator for causal LM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # False because this is causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data collator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9608f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer ready\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc88e624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='267' max='267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [267/267 24:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.323900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.532400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training complete and LoRA adapter saved\n"
     ]
    }
   ],
   "source": [
    "# Begin training\n",
    "trainer.train()\n",
    "\n",
    "# Save the LoRA adapter after training\n",
    "model.save_pretrained(\"outputs/lora_adapter\")\n",
    "\n",
    "print(\"‚úÖ Training complete and LoRA adapter saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a49f5",
   "metadata": {},
   "source": [
    "# Part 3: Quick Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e75c0257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fe069b46584bd2be43567390b2b9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded from safetensor shards\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# --- Paths ---\n",
    "model_path = \"./outputs/lora_adapter\"  # folder with your safetensor shards\n",
    "tokenizer_path = \"meta-llama/Llama-3.1-8B\"  # or same folder if tokenizer saved locally\n",
    "\n",
    "# --- Load tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # ensure padding token is set\n",
    "\n",
    "# --- Load full model ---\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",     # automatically place layers on GPUs\n",
    "    torch_dtype=torch.bfloat16,  # use bfloat16 for efficiency\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded from safetensor shards\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b1d0467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Write a political news story from a right perspective based on the headline.\n",
      "\n",
      "### Input:\n",
      "Headline: Georgia Tech student working on NLP project\n",
      "\n",
      "### Response:\n",
      "This material may not be published, broadcast, rewritten, or redistributed. ¬©2025 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided byFactset. Powered and implemented byFactSet Digital Solutions.Legal Statement. Mutual Fund and ETF data provided byRefinitiv Lipper.\n",
      "\n",
      "Fox News Flash top headlines are here. Check out what's clicking on Foxnews.com.\n",
      "\n",
      "AGeorgiahigh school student who is working on a \"nonpartisan\"artificial intelligence (AI) project with a \"federal agency\" has sparked concerns over the privacy and security of the teen's personal data.\n",
      "\n",
      "The teen, who is in his 20s, is a junior at Georgetown University's Center for Global and Social Justice Science. He has been on the cusp of completing his work on the ChatGPT-4R chatbot, a project that has garnered national attention and scrutiny, with some critics arguing that the teen's work on the chatbot is \"deeply concerning.\"\n",
      "\n",
      "\"I think it‚Äôs very clear this is a federal agency that is using state-level data and biometric information to control and influence a population of Americans,\" Adam Freifeld, an Atlanta-based defense attorney whose 11-year-old son is working on the chatbot, told Fox News Digital.\n",
      "\n",
      "\"This is something that is deeply concerning, and there is a federal agency using this to advance a political agenda,\" he added. \"There are very serious\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Test generation ---\n",
    "prompt = \"\"\"### Instruction:\n",
    "Write a political news story from a right perspective based on the headline.\n",
    "\n",
    "### Input:\n",
    "Headline: Georgia Tech student working on NLP project\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output_ids = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc34c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
