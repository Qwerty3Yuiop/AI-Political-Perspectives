{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09c86bd",
   "metadata": {},
   "source": [
    "# Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf7fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PEFT along with dependencies\n",
    "!pip install -q peft transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b1ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import platform\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb53688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME\n",
      "/home/hice1/yhsu72/scratch/hf_cache\n",
      "TRITON_CACHE_DIR\n",
      "/home/hice1/yhsu72/scratch/triton_cache\n",
      "TORCHINDUCTOR_CACHE_DIR\n",
      "/home/hice1/yhsu72/scratch/inductor_cache\n",
      "NLTK_DATA\n",
      "/home/hice1/yhsu72/scratch/nltk_data\n",
      "Make sure the cache files are in ~/scratch/ so quota doesn't exceed limit!\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL ONLY IF RUNNING ON PACE-ICE\n",
    "\n",
    "\n",
    "# override the huggingface cache path and nltk cache path\n",
    "dirs = {\n",
    "    \"HF_HOME\":\"~/scratch/hf_cache\",\n",
    "    \"TRITON_CACHE_DIR\":\"~/scratch/triton_cache\",\n",
    "    \"TORCHINDUCTOR_CACHE_DIR\":\"~/scratch/inductor_cache\",\n",
    "    'NLTK_DATA':\"~/scratch/nltk_data\"\n",
    "}\n",
    "\n",
    "for name in dirs:\n",
    "    d = dirs[name]\n",
    "    path = os.path.expanduser(d)\n",
    "    print(name)\n",
    "    print(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    # making sure the cache dirs are rwx for owner\n",
    "    os.chmod(path, 0o700)\n",
    "    os.environ[name] = path\n",
    "print(\"Make sure the cache files are in ~/scratch/ so quota doesn't exceed limit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your own token (or save in .env)\n",
    "os.environ['HF_TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e2255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hugging Face token loaded from environment.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # loads HF_TOKEN into environment\n",
    "\n",
    "print(\"‚úÖ Hugging Face token loaded from environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f31a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== üß† Environment Info ===\n",
      "Python version: 3.10.13\n",
      "PyTorch version: 2.8.0+cu128\n",
      "-----------------------------\n",
      "‚úÖ CUDA is available. Number of GPUs: 2\n",
      "  ‚Ä¢ GPU 0: NVIDIA H200 (139.80 GB VRAM)\n",
      "  ‚Ä¢ GPU 1: NVIDIA H200 (139.80 GB VRAM)\n",
      "\n",
      "Using GPU: NVIDIA H200\n",
      "Available VRAM: 149.56 GB / 150.11 GB\n",
      "-----------------------------\n",
      "Default torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"=== üß† Environment Info ===\")\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# Check for CUDA (NVIDIA GPUs)\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"‚úÖ CUDA is available. Number of GPUs: {num_gpus}\")\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        print(f\"  ‚Ä¢ GPU {i}: {gpu_name} ({total_mem:.2f} GB VRAM)\")\n",
    "\n",
    "    # Also show current GPU and free memory\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    print(f\"\\nUsing GPU: {torch.cuda.get_device_name(current_gpu)}\")\n",
    "    free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "    print(f\"Available VRAM: {free_mem/1e9:.2f} GB / {total_mem/1e9:.2f} GB\")\n",
    "\n",
    "# Check for Apple Silicon (MPS)\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"‚úÖ Running on Apple Silicon (MPS backend).\")\n",
    "\n",
    "# Check for ROCm (AMD GPUs)\n",
    "elif torch.version.hip is not None:\n",
    "    print(\"‚úÖ ROCm (AMD GPU) detected.\")\n",
    "\n",
    "# Otherwise fallback to CPU\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected ‚Äî running on CPU only.\")\n",
    "    print(\"This will be very slow for large models like Llama-3.1-8B.\")\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# Confirm torch default device\n",
    "default_device = \"cuda\" if torch.cuda.is_available() else (\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Default torch device: {default_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0d83b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model‚Ä¶\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c90621fcffa463683cc18575a46b251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Model name on Hugging Face Hub ---\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# --- 4. (Optional) Authenticate if model is gated/private ---\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"YOUR_HF_TOKEN\")\n",
    "\n",
    "print(\"Loading tokenizer and model‚Ä¶\")\n",
    "\n",
    "# --- 5. Load tokenizer ---\n",
    "# Tokenizer converts text ‚Üî tokens. Must match model for correct vocabulary.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- 6. Define quantization configuration ---\n",
    "# This allows loading the model in 4-bit precision to save VRAM and enable QLoRA fine-tuning.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Quantize model weights to 4 bits instead of 16\n",
    "    bnb_4bit_quant_type=\"nf4\",              # \"NormalFloat4\" ‚Äì more accurate 4-bit representation\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for math (fast, widely supported)\n",
    "    bnb_4bit_use_double_quant=True          # Extra quantization layer to reduce memory further\n",
    ")\n",
    "\n",
    "# --- 7. Load model with efficient settings ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",              # Automatically distributes layers across GPUs/CPU\n",
    "    quantization_config=bnb_config, # Apply the quantization config defined above\n",
    "    dtype=torch.bfloat16,     # Keep remaining layers in bfloat16 precision (safe default)\n",
    "    low_cpu_mem_usage=True,         # Stream weights directly to GPU to reduce CPU RAM footprint\n",
    "    trust_remote_code=True          # Needed if the repo includes custom model code\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7a0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Explain the difference between left-wing and right-wing economic policies.\n",
      "\n",
      "### Response: \n",
      "The main differences are in how government should regulate business. The Left believes that businesses need to be regulated heavily by a central authority, so as not to harm workers or consumers (e.g., minimum wage laws). Meanwhile, the Right generally opposes such regulations on principle because they think it is better for people to make their own decisions about wages etc...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 8. Simple inference test ---\n",
    "prompt = \"\"\"### Instruction:\n",
    "Explain the difference between left-wing and right-wing economic policies.\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,                  # control output length\n",
    "    do_sample=True,                      # enables some randomness\n",
    "    temperature=0.7,                     # mild creativity\n",
    "    top_p=0.9,                           # nucleus sampling\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.2               # prevent repeated text\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165a05c",
   "metadata": {},
   "source": [
    "# Part 1: PEFT Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e6b975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                # rank of the LoRA matrices\n",
    "    lora_alpha=32,       # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # which layers to fine-tune\n",
    "    lora_dropout=0.05,   # dropout for LoRA\n",
    "    bias=\"none\",         # keep bias frozen\n",
    "    task_type=\"CAUSAL_LM\" # type of task\n",
    ")\n",
    "\n",
    "# Wrap base model with PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # confirm only LoRA params are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4a622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
