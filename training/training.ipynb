{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09c86bd",
   "metadata": {},
   "source": [
    "# Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf7fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Install PEFT along with dependencies\n",
    "!pip install -q peft transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b1ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import platform\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb53688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME\n",
      "/home/hice1/jho89/scratch/hf_cache\n",
      "TRITON_CACHE_DIR\n",
      "/home/hice1/jho89/scratch/triton_cache\n",
      "TORCHINDUCTOR_CACHE_DIR\n",
      "/home/hice1/jho89/scratch/inductor_cache\n",
      "NLTK_DATA\n",
      "/home/hice1/jho89/scratch/nltk_data\n",
      "Make sure the cache files are in ~/scratch/ so quota doesn't exceed limit!\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL ONLY IF RUNNING ON PACE-ICE\n",
    "# override the huggingface cache path and nltk cache path\n",
    "dirs = {\n",
    "    \"HF_HOME\":\"~/scratch/hf_cache\",\n",
    "    \"TRITON_CACHE_DIR\":\"~/scratch/triton_cache\",\n",
    "    \"TORCHINDUCTOR_CACHE_DIR\":\"~/scratch/inductor_cache\",\n",
    "    'NLTK_DATA':\"~/scratch/nltk_data\"\n",
    "}\n",
    "\n",
    "for name in dirs:\n",
    "    d = dirs[name]\n",
    "    path = os.path.expanduser(d)\n",
    "    print(name)\n",
    "    print(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    # making sure the cache dirs are rwx for owner\n",
    "    os.chmod(path, 0o700)\n",
    "    os.environ[name] = path\n",
    "print(\"Make sure the cache files are in ~/scratch/ so quota doesn't exceed limit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1125dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your own token (or save in .env)\n",
    "os.environ['HF_TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e2255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hugging Face token loaded from environment.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # loads HF_TOKEN into environment\n",
    "print(\"‚úÖ Hugging Face token loaded from environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f31a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== üß† Environment Info ===\n",
      "Python version: 3.10.13\n",
      "PyTorch version: 2.8.0+cu128\n",
      "-----------------------------\n",
      "‚úÖ CUDA is available. Number of GPUs: 1\n",
      "  ‚Ä¢ GPU 0: NVIDIA H200 (139.80 GB VRAM)\n",
      "\n",
      "Using GPU: NVIDIA H200\n",
      "Available VRAM: 101.23 GB / 150.11 GB\n",
      "-----------------------------\n",
      "Default torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"=== üß† Environment Info ===\")\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# Check for CUDA (NVIDIA GPUs)\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"‚úÖ CUDA is available. Number of GPUs: {num_gpus}\")\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        print(f\"  ‚Ä¢ GPU {i}: {gpu_name} ({total_mem:.2f} GB VRAM)\")\n",
    "\n",
    "    # Also show current GPU and free memory\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    print(f\"\\nUsing GPU: {torch.cuda.get_device_name(current_gpu)}\")\n",
    "    free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "    print(f\"Available VRAM: {free_mem/1e9:.2f} GB / {total_mem/1e9:.2f} GB\")\n",
    "\n",
    "# Check for Apple Silicon (MPS)\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"‚úÖ Running on Apple Silicon (MPS backend).\")\n",
    "\n",
    "# Check for ROCm (AMD GPUs)\n",
    "elif torch.version.hip is not None:\n",
    "    print(\"‚úÖ ROCm (AMD GPU) detected.\")\n",
    "\n",
    "# Otherwise fallback to CPU\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected ‚Äî running on CPU only.\")\n",
    "    print(\"This will be very slow for large models like Llama-3.1-8B.\")\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# Confirm torch default device\n",
    "default_device = \"cuda\" if torch.cuda.is_available() else (\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Default torch device: {default_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa0d83b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Model name on Hugging Face Hub ---\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# --- 4. (Optional) Authenticate if model is gated/private ---\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"YOUR_HF_TOKEN\")\n",
    "\n",
    "print(\"Loading tokenizer and model‚Ä¶\")\n",
    "\n",
    "# --- 5. Load tokenizer ---\n",
    "# Tokenizer converts text ‚Üî tokens. Must match model for correct vocabulary.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # ensure padding works\n",
    "\n",
    "# --- 6. Load model in full bf16 precision (no quantization) ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",      # Automatically distributes layers across GPUs\n",
    "    torch_dtype=torch.bfloat16,  # Use bf16 for all layers\n",
    "    low_cpu_mem_usage=True,       # Stream weights directly to GPU to reduce CPU RAM footprint\n",
    "    trust_remote_code=True        # Needed if the repo includes custom code\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f7a0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Explain the difference between left-wing and right-wing economic policies.\n",
      "\n",
      "### Response: \n",
      "The main differences in economic policy between these two ideologies are that Left-Wing ideology tends to support government intervention, regulation of markets by taxation, redistribution of income or wealth through progressive tax systems and social welfare programs such as unemployment benefits etc., whereas Right Wing Economics focuses more on individual responsibility rather than collective action with less emphasis placed upon state control over private enterprise activities. Additionally there may also be ideological variations depending on which side you look at e.g some people consider themselves libertarian but still identify strongly enough within one camp so they can't really fit into either category perfectly well - this sorta thing happens quite often!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 8. Simple inference test ---\n",
    "prompt = \"\"\"### Instruction:\n",
    "Explain the difference between left-wing and right-wing economic policies.\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,                  # control output length\n",
    "    do_sample=True,                      # enables some randomness\n",
    "    temperature=0.7,                     # mild creativity\n",
    "    top_p=0.9,                           # nucleus sampling\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.2               # prevent repeated text\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165a05c",
   "metadata": {},
   "source": [
    "# Part 1: Load & Prepare Data\n",
    "Data when loaded in:\n",
    "```\n",
    "{\n",
    "    \"instruction\": \"Write a political news story from a {left/ right/ center} perspective based on the headline.\", \n",
    "    \"input\": \"Headline: ...\",\n",
    "    \"output\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Steps to process it:**\n",
    "1. Combine into 1 traning text\n",
    "2. Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfb5e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded train.jsonl successfully!\n",
      "Number of samples: 2852\n",
      "\n",
      "üìä Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a political news story from a right pers...</td>\n",
       "      <td>Headline: If Democrats Flip House What Will Th...</td>\n",
       "      <td>Quotes displayed in real-time or delayed by at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a political news story from a center per...</td>\n",
       "      <td>Headline: Putin Marks Russias Victory Day Spee...</td>\n",
       "      <td>It has become an annual event - the military p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write a political news story from a left persp...</td>\n",
       "      <td>Headline: Media Industry Ap Criticized After D...</td>\n",
       "      <td>Copyright 2025 The Associated Press. All Right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Write a political news story from a left persp...</td>\n",
       "      <td>Headline: Economy And Jobs Will There Be Reces...</td>\n",
       "      <td>As the Fed wrestles with inflation, experts wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write a political news story from a right pers...</td>\n",
       "      <td>Headline: Technology Plans Tesla Tunnel Nashvi...</td>\n",
       "      <td>The project will be similar to one already in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Write a political news story from a right pers...   \n",
       "1  Write a political news story from a center per...   \n",
       "2  Write a political news story from a left persp...   \n",
       "3  Write a political news story from a left persp...   \n",
       "4  Write a political news story from a right pers...   \n",
       "\n",
       "                                               input  \\\n",
       "0  Headline: If Democrats Flip House What Will Th...   \n",
       "1  Headline: Putin Marks Russias Victory Day Spee...   \n",
       "2  Headline: Media Industry Ap Criticized After D...   \n",
       "3  Headline: Economy And Jobs Will There Be Reces...   \n",
       "4  Headline: Technology Plans Tesla Tunnel Nashvi...   \n",
       "\n",
       "                                              output  \n",
       "0  Quotes displayed in real-time or delayed by at...  \n",
       "1  It has become an annual event - the military p...  \n",
       "2  Copyright 2025 The Associated Press. All Right...  \n",
       "3  As the Fed wrestles with inflation, experts wo...  \n",
       "4  The project will be similar to one already in ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# build full paths\n",
    "data_path = os.path.join(cwd, \"train.jsonl\")   # input file\n",
    "# Option 1: Load using pandas ‚Äî easiest for inspection\n",
    "train_df = pd.read_json(data_path, lines=True)\n",
    "\n",
    "print(\"‚úÖ Loaded train.jsonl successfully!\")\n",
    "print(f\"Number of samples: {len(train_df)}\\n\")\n",
    "print(\"üìä Preview:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e536f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    \"\"\"\n",
    "    Combine instruction, input, and output into one training text.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        f\"### Instruction:\\n{example['instruction'].strip()}\\n\\n\"\n",
    "        f\"### Input:\\n{example['input'].strip()}\\n\\n\"\n",
    "        f\"### Response:\\n{example['output'].strip()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d764f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Write a political news story from a left perspective based on the headline.\n",
      "\n",
      "### Input:\n",
      "Headline: Perspectives Florida Bans Critical Race Theory Schools, Summary: The Florida State Board of Education unanimously voted to ban lessons that include critical race theory (CRT) from public schools Thursday. The board approved anamendment that¬†prohibitsthe teaching that America is inherently racist, or¬†\"the theory that racism is not merely the product of prejudice, but that racism is embedded in American society and its legal systems in order to uphold the supremacy of white persons.\" Florida Gov. Ron DeSantis said in a video before the meeting that¬†CRT distorts history, and urged board members, many who were appointed by DeSantis himself,¬†to approve of the amendment that would teach student historical¬†facts instead of ‚Äútrying to indoctrinate them with ideology.‚Äù This comes after other states, such as¬†Idaho, Oklahoma, Tennessee, and Iowa, have putrestrictions on CRT in classrooms.¬†Critics of the theory say that students should not be taught that the U.S. is inherently racist, while advocates of CRT argue the¬†U.S. was founded on stolen land and slave labor, and many of the country's¬†institutions still uphold racism through laws and systems that favor white people over people of color. People on both of the argument also warn that¬†narrowing acceptable classroom material could violate free speech.Many on the left criticized DeSantis, calling the ban's language \"vague\" and arguing that¬†the ban is aimed atavoiding analysis of ongoing racism. Right-rated outlets tended to focus on DeSantis' speech and the details of the amendment, and frame CRT in a negative light.\n",
      "\n",
      "### Response:\n",
      "\n",
      "-------\n",
      "Copyright 2025 The Associated Press. All Rights Reserved.\n",
      "\n",
      "Copyright 2025 The Associated Press. All Rights Reserved.\n",
      "\n",
      "FILE - In this April 30, 2021, file photo surrounded by lawmakers, Florida Gov.Ron DeSantis speaks at the end of a legislative session at the Capitol in Tallahassee, Fla. Now that the pandemic appears to be waning and DeSantis is heading into his reelection campaign next year, he h\n"
     ]
    }
   ],
   "source": [
    "N = 200\n",
    "eval_df = train_df.sample(n=N, random_state=42).reset_index(drop=True)\n",
    "\n",
    "def build_eval_prompt(row):\n",
    "    \"\"\"\n",
    "    Same format as training, but without including the output.\n",
    "    The model should generate the part after '### Response:'.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        f\"### Instruction:\\n{row['instruction'].strip()}\\n\\n\"\n",
    "        f\"### Input:\\n{row['input'].strip()}\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "eval_prompts = [build_eval_prompt(r) for _, r in eval_df.iterrows()]\n",
    "eval_references = [r[\"output\"] for _, r in eval_df.iterrows()]  # gold answers\n",
    "print(eval_prompts[0])\n",
    "print(\"-------\")\n",
    "print(eval_references[0][:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c37608d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Write a political news story from a right perspective based on the headline.\n",
      "\n",
      "### Input:\n",
      "Headline: If Democrats Flip House What Will They Prioritize, Summary: With Democrats largely projected to flip the House, perspectives vary on their potential agenda, with some positing they will reform campaign ethics, outlaw gerrymandering, and bolster voting rights, while others say they will prioritize impeachment and increase spending. Republicans are expected to keep the Senate, so some also point out that many policy initiatives on both sides of the aisle will likely come to a standstill.\n",
      "\n",
      "### Response:\n",
      "Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided byFactset.\n",
      "          Powered and implemented byFactSet Digital Solutions.Legal Statement.\n",
      "\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "# Apply formatting to DataFrames\n",
    "train_df[\"text\"] = train_df.apply(format_prompt, axis=1)\n",
    "\n",
    "# Preview one formatted sample\n",
    "print(train_df[\"text\"].iloc[0][:800])  # show first 800 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29704e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2852/2852 [00:05<00:00, 511.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2852\n",
      "})\n",
      "============ Sanity Check - decoding encoded tokens ============\n",
      "<|begin_of_text|>### Instruction:\n",
      "Write a political news story from a right perspective based on the headline.\n",
      "\n",
      "### Input:\n",
      "Headline: If Democrats Flip House What Will They Prioritize, Summary: With Democrats largely projected to flip the House, perspectives vary on their potential agenda, with some positing they will reform campaign ethics, outlaw gerrymandering, and bolster voting rights, while others say they will prioritize impeachment and increase spending. Republicans are expected to keep the Senate, so some also point out that many policy initiatives on both sides of the aisle will likely come to a standstill.\n",
      "\n",
      "### Response:\n",
      "Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided byFactset.\n",
      "          Powered and implemented byFactSet Digital Solutions.Legal Statement.\n",
      "\n",
      "This material may not be published, broadcast, rewritten, or redistributed. ¬©2025 FOX News Network, LLC. All rights reserved.FAQ-New Privacy Policy\n",
      "\n",
      "If the Democrats win a majority in theHouse, it will be Democrats who chair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,   # adjust based on GPU memory\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "# Convert to HF Dataset and tokenize\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\"]])\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Check shape and sample\n",
    "print(tokenized_train)\n",
    "\n",
    "print(\"============ Sanity Check - decoding encoded tokens ============\")\n",
    "print(tokenizer.decode(tokenized_train[0][\"input_ids\"][:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaffa87",
   "metadata": {},
   "source": [
    "# Part 2: LoRA fine-tuning\n",
    "**Steps**\n",
    "1. Set up LoRA model\n",
    "2. Set up training arguments\n",
    "3. Prepare a Data Collator\n",
    "4. Set up Trainer\n",
    "5. Run Training\n",
    "6. Save new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84e6b975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "# Set up Lora model for fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                # rank of the LoRA matrices\n",
    "    lora_alpha=32,       # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # which layers to fine-tune\n",
    "    lora_dropout=0.05,   # dropout for LoRA\n",
    "    bias=\"none\",         # keep bias frozen\n",
    "    task_type=\"CAUSAL_LM\" # type of task\n",
    ")\n",
    "\n",
    "# Wrap base model with PEFT\n",
    "model = get_peft_model(model, lora_config) # freezes original layer\n",
    "model.print_trainable_parameters()  # confirm only LoRA params are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bb4a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments set up\n"
     ]
    }
   ],
   "source": [
    "# Define Training Arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/lora\",         # where to save checkpoints & LoRA adapters\n",
    "    per_device_train_batch_size=8,     # batch size per GPU\n",
    "    gradient_accumulation_steps=4,     # effective batch size = 8 * 4 * 2 GPUs = 64\n",
    "    learning_rate=3e-4,                # LoRA-friendly default\n",
    "    num_train_epochs=3,                # 3 epochs, increase if dataset is larger\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,                          # optional: H200 supports bf16, can improve speed\n",
    "    dataloader_drop_last=True,           # drop incomplete batch to avoid OOM\n",
    "    report_to=\"none\",                    # change to 'wandb' if using W&B\n",
    "    remove_unused_columns=False,\n",
    "    ddp_find_unused_parameters=False,   # improves multi-GPU performance\n",
    "    gradient_checkpointing=True,        # reduce memory usage for large models\n",
    "    warmup_steps=50,                    # optional warmup\n",
    "    optim=\"paged_adamw_32bit\",          # memory-efficient optimizer\n",
    "    lr_scheduler_type=\"cosine\"          # smooth LR schedule\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training arguments set up\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31533576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data collator ready\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data Collator (Coverts list of examples into tensors)\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Collator for causal LM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # False because this is causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data collator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9608f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer ready\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88e624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='266' max='267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [266/267 17:08 < 00:03, 0.26 it/s, Epoch 2.98/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.794800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.712300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.702200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Begin training\n",
    "trainer.train()\n",
    "\n",
    "# Save the LoRA adapter after training\n",
    "model.save_pretrained(\"outputs/lora_adapter\")\n",
    "\n",
    "print(\"‚úÖ Training complete and LoRA adapter saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a49f5",
   "metadata": {},
   "source": [
    "# Part 3: Quick Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75c0257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model name: meta-llama/Llama-3.1-8B\n",
      "Adapter path: ./outputs/lora_adapter exists? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded base model + LoRA adapter; ready for evaluation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/jho89/.local/lib/python3.10/site-packages/peft/peft_model.py:598: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch, os\n",
    "\n",
    "# ---- Paths ----\n",
    "# base model you used during LoRA training\n",
    "base_model_name = \"meta-llama/Llama-3.1-8B\"   # change if you used a different one\n",
    "\n",
    "# absolute path to your LoRA adapter (from the screenshot)\n",
    "adapter_path = \"./outputs/lora_adapter\"\n",
    "\n",
    "print(\"Base model name:\", base_model_name)\n",
    "print(\"Adapter path:\", adapter_path, \"exists?\", os.path.isdir(adapter_path))\n",
    "\n",
    "# ---- Load tokenizer from base model ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ---- Load base model ----\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ---- Attach LoRA adapter on top of base model ----\n",
    "trained_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    ")\n",
    "\n",
    "# Optional: merge LoRA weights into the base model and drop the adapter structure\n",
    "# trained_model = trained_model.merge_and_unload()\n",
    "\n",
    "print(\"‚úÖ Loaded base model + LoRA adapter; ready for evaluation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b1d0467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Write a political news story from a right perspective based on the headline.\n",
      "\n",
      "### Input:\n",
      "Headline: Georgia Tech student working on NLP project\n",
      "\n",
      "### Response:\n",
      "Georgia Tech student working on NLP project is an example of how technology is being used to advance the field of artificial intelligence. The project, which is being developed by a team of students at the university, aims to create a system that can understand and respond to human language in a more natural and intuitive way. This could have a number of applications, including improving customer service, developing more advanced virtual assistants, and creating more sophisticated chatbots. The project is still in its early stages, but the team is hopeful that it will be able to make significant progress in the coming years.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Test generation ---\n",
    "prompt = \"\"\"### Instruction:\n",
    "Write a political news story from a right perspective based on the headline.\n",
    "\n",
    "### Input:\n",
    "Headline: Georgia Tech student working on NLP project\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output_ids = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fc34c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [16:02<00:00,  4.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "trained_model.to(device)\n",
    "\n",
    "generated_outputs = []\n",
    "\n",
    "for i, prompt in tqdm(enumerate(eval_prompts), total=len(eval_prompts), desc=\"Generating\"):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = trained_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_tokens = output_ids[0, input_len:]\n",
    "    gen_text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    generated_outputs.append(gen_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caf2f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to generated_outputs.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"generated_outputs.jsonl\", \"w\") as f:\n",
    "    for prompt, output in zip(eval_prompts, generated_outputs):\n",
    "        f.write(json.dumps({\"prompt\": prompt, \"generation\": output}) + \"\\n\")\n",
    "\n",
    "print(\"Saved to generated_outputs.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001f381",
   "metadata": {},
   "source": [
    "Distinct N evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b2d611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct-1: 0.2067\n",
      "Distinct-2: 0.6128\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def distinct_n(corpus, n=1):\n",
    "    all_ngrams = []\n",
    "    for text in corpus:\n",
    "        tokens = text.split()\n",
    "        if len(tokens) < n:\n",
    "            continue\n",
    "        all_ngrams.extend(get_ngrams(tokens, n))\n",
    "\n",
    "    if not all_ngrams:\n",
    "        return 0.0\n",
    "\n",
    "    unique_ngrams = set(all_ngrams)\n",
    "    return len(unique_ngrams) / len(all_ngrams)\n",
    "\n",
    "d1 = distinct_n(generated_outputs, n=1)\n",
    "d2 = distinct_n(generated_outputs, n=2)\n",
    "\n",
    "print(f\"Distinct-1: {d1:.4f}\")\n",
    "print(f\"Distinct-2: {d2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c34fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_texts = [str(x) for x in eval_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c8a3985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 prompt‚Üîgeneration similarities: [0.4634, 0.0702, 0.948, 0.9443, 0.9645, 0.4293, 0.9097, 0.9684, 0.8727, 0.8349]\n",
      "\n",
      "Average semantic similarity: 0.8032\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sem_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "emb_prompts = sem_model.encode(prompt_texts, convert_to_tensor=True)\n",
    "emb_gen     = sem_model.encode(generated_outputs, convert_to_tensor=True)\n",
    "\n",
    "cos_scores = util.cos_sim(emb_prompts, emb_gen).diagonal()\n",
    "\n",
    "avg_sim = float(cos_scores.mean())\n",
    "print(\"First 10 prompt‚Üîgeneration similarities:\",\n",
    "      [round(float(x), 4) for x in cos_scores[:10]])\n",
    "print(f\"\\nAverage semantic similarity: {avg_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2f52505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: /storage/ice1/3/6/jho89/AI-Political-Perspectives-main/training/eval_results.jsonl\n",
      "Saved 200 records with similarity to eval_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "output_path = \"eval_results.jsonl\"\n",
    "print(\"Saving to:\", os.path.abspath(output_path))\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for prompt, generation, sim in zip(eval_prompts, generated_outputs, cos_scores):\n",
    "        record = {\n",
    "            \"prompt\": prompt,\n",
    "            \"generation\": generation,\n",
    "            \"semantic_similarity\": float(sim),\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved\", len(generated_outputs), \"records with similarity to\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0237af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_nlp_hw1_env",
   "language": "python",
   "name": "nlp-hw1-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
